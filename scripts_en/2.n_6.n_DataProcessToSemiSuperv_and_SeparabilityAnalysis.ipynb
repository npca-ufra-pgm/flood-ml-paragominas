{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f8db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file to examine data\n",
    "file_path = '/content/drive/MyDrive/floodProjectAssets/datasetFlood_toDrive.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first rows of the file to see its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e8e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select relevant columns for correlation matrix and reorder\n",
    "data_for_analysis = data.drop(columns=['system:index', 'landcover_classification_2022', '.geo'])\n",
    "column_order = ['distance', 'elevation', 'slope', 'soil_hydraulic_conductivity', 'hand', 'twi', 'classes']\n",
    "data_for_analysis = data_for_analysis[column_order]\n",
    "\n",
    "# Visualize distributions of numerical features\n",
    "data_for_analysis.hist(bins=30, figsize=(15, 10))\n",
    "plt.suptitle('Distribution of Numerical Features (before data balancing)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize relationships between features using pairplot\n",
    "sns.pairplot(data_for_analysis, hue='classes', diag_kind='kde')\n",
    "plt.suptitle('Pairplot of Features by Class (before data balancing)', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Visualize boxplots for each feature by class\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(data_for_analysis.columns[:-1]): # Exclude the target variable for individual boxplots\n",
    "    plt.subplot(3, 3, i + 1) # Changed from 2x2 to 3x3 grid\n",
    "    sns.boxplot(x='classes', y=col, data=data_for_analysis)\n",
    "    plt.title(f'Boxplot of {col} by Class')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the Pearson correlation matrix for all classes\n",
    "correlation_matrix_all = data_for_analysis.drop(columns=['classes']).corr(method='pearson')\n",
    "\n",
    "# Display the correlation matrix for all classes\n",
    "print(\"Pearson Correlation Matrix for all classes:\")\n",
    "display(correlation_matrix_all)\n",
    "\n",
    "# Visualize the correlation matrix for all classes as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix_all, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Pearson Correlation Matrix - All Classes (before data balancing)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate Pearson correlation matrix for class 0\n",
    "correlation_matrix_class_0 = data_for_analysis[data_for_analysis['classes'] == 0].drop(columns=['classes']).corr(method='pearson')\n",
    "\n",
    "# Display correlation matrix for class 0\n",
    "print(\"Pearson Correlation Matrix for Class 0:\")\n",
    "display(correlation_matrix_class_0)\n",
    "\n",
    "# Visualize correlation matrix for class 0 as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix_class_0, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Pearson Correlation Matrix - Class 0 (before data balancing)')\n",
    "plt.show()\n",
    "\n",
    "# Calculate Pearson correlation matrix for class 1\n",
    "correlation_matrix_class_1 = data_for_analysis[data_for_analysis['classes'] == 1].drop(columns=['classes']).corr(method='pearson')\n",
    "\n",
    "# Display correlation matrix for class 1\n",
    "print(\"\\nPearson Correlation Matrix for Class 1:\")\n",
    "display(correlation_matrix_class_1)\n",
    "\n",
    "# Visualize correlation matrix for class 1 as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix_class_1, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Pearson Correlation Matrix - Class 1 (before data balancing)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca041064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# All classes\n",
    "X_vif = data_for_analysis.drop(columns=['classes'])\n",
    "X_vif = sm.add_constant(X_vif)  # <── add constant\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i)\n",
    "                   for i in range(X_vif.shape[1])]\n",
    "vif_data = vif_data[vif_data[\"feature\"] != \"const\"]  # remove constant from table\n",
    "print(\"Variance Inflation Factor (VIF) for all features:\")\n",
    "display(vif_data)\n",
    "\n",
    "# Class 0\n",
    "X_vif_class_0 = data_for_analysis[data_for_analysis['classes'] == 0].drop(columns=['classes'])\n",
    "X_vif_class_0 = sm.add_constant(X_vif_class_0)\n",
    "\n",
    "vif_data_class_0 = pd.DataFrame()\n",
    "vif_data_class_0[\"feature\"] = X_vif_class_0.columns\n",
    "vif_data_class_0[\"VIF\"] = [variance_inflation_factor(X_vif_class_0.values, i)\n",
    "                           for i in range(X_vif_class_0.shape[1])]\n",
    "vif_data_class_0 = vif_data_class_0[vif_data_class_0[\"feature\"] != \"const\"]\n",
    "print(\"\\nVariance Inflation Factor (VIF) for features in Class 0:\")\n",
    "display(vif_data_class_0)\n",
    "\n",
    "# Class 1\n",
    "X_vif_class_1 = data_for_analysis[data_for_analysis['classes'] == 1].drop(columns=['classes'])\n",
    "X_vif_class_1 = sm.add_constant(X_vif_class_1)\n",
    "\n",
    "vif_data_class_1 = pd.DataFrame()\n",
    "vif_data_class_1[\"feature\"] = X_vif_class_1.columns\n",
    "vif_data_class_1[\"VIF\"] = [variance_inflation_factor(X_vif_class_1.values, i)\n",
    "                           for i in range(X_vif_class_1.shape[1])]\n",
    "vif_data_class_1 = vif_data_class_1[vif_data_class_1[\"feature\"] != \"const\"]\n",
    "print(\"\\nVariance Inflation Factor (VIF) for features in Class 1:\")\n",
    "display(vif_data_class_1)\n",
    "\n",
    "\n",
    "'''VIF Value Interpretation\n",
    "1       No correlation with other variables\n",
    "1–5     Moderate correlation, generally acceptable\n",
    "> 5     Concerning multicollinearity\n",
    "> 10    Very high – should remove or combine variables'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc697dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Generate basic statistics for the entire data_for_analysis DataFrame\n",
    "print(\"Basic Statistics for the entire dataset:\")\n",
    "display(data_for_analysis.describe())\n",
    "\n",
    "# Generate basic statistics for Class 0\n",
    "print(\"\\nBasic Statistics for Class 0:\")\n",
    "display(data_for_analysis[data_for_analysis['classes'] == 0].describe())\n",
    "\n",
    "# Generate basic statistics for Class 1\n",
    "print(\"\\nBasic Statistics for Class 1:\")\n",
    "display(data_for_analysis[data_for_analysis['classes'] == 1].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c695992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate attributes and classes\n",
    "X = data.drop(columns=['system:index', 'landcover_classification_2022', 'classes', '.geo'])  # Remove non-numeric columns\n",
    "y = data['classes']\n",
    "\n",
    "# Apply SMOTE to increase minority class samples by 50%\n",
    "smote = SMOTE(sampling_strategy=0.2, random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# Check class distribution before and after SMOTE\n",
    "y.value_counts(), y_res.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Separate data into labeled (class 1) and unlabeled (class 0)\n",
    "X_labeled = X_res[y_res == 1]\n",
    "y_labeled = y_res[y_res == 1]\n",
    "\n",
    "X_unlabeled = X_res[y_res == 0]\n",
    "y_unlabeled = y_res[y_res == 0]\n",
    "\n",
    "# Select 20% of labeled samples as \"spies\"\n",
    "X_spy, X_labeled_train, y_spy, y_labeled_train = train_test_split(\n",
    "    X_labeled, y_labeled, test_size=0.8, random_state=42, stratify=y_labeled\n",
    ")\n",
    "\n",
    "print(\"Shape of y_spy:\", y_spy.shape)\n",
    "print(\"Shape of y_labeled_train:\", y_labeled_train.shape)\n",
    "print(\"Shape of y_unlabeled:\", y_unlabeled.shape)\n",
    "\n",
    "y_spy_fake = pd.Series([0] * len(y_spy), index=y_spy.index)\n",
    "\n",
    "# Mix \"spies\" with unlabeled data\n",
    "X_train_spy = pd.concat([X_labeled_train, X_unlabeled, X_spy])\n",
    "y_train_spy = pd.concat([y_labeled_train, y_unlabeled, y_spy_fake])\n",
    "\n",
    "# Shuffle data\n",
    "shuffle_index = np.random.permutation(len(X_train_spy))\n",
    "X_train_spy = X_train_spy.iloc[shuffle_index].reset_index(drop=True)\n",
    "y_train_spy = y_train_spy.iloc[shuffle_index].reset_index(drop=True)\n",
    "\n",
    "print('--------------')\n",
    "print(\"Shape of X_train_spy:\", X_train_spy.shape)\n",
    "print(\"Shape of y_train_spy:\", y_train_spy.shape)\n",
    "print(\"\\nValue counts of y_train_spy:\")\n",
    "print(y_train_spy.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Initialize Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "# Train classifier with training data, including spies\n",
    "nb_classifier.fit(X_train_spy, y_train_spy)\n",
    "\n",
    "# Calculate probabilities for training data\n",
    "proba_spy = nb_classifier.predict_proba(X_spy)\n",
    "\n",
    "# Display first probabilities\n",
    "print(\"Probabilities calculated by Naive Bayes:\")\n",
    "print(proba_spy[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a8ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_positive = proba_spy[:, 1]\n",
    "print(\"Probabilities of positive class:\")\n",
    "print(proba_positive[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1cdf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean = np.mean(proba_positive)\n",
    "std_dev = np.std(proba_positive)\n",
    "median = np.median(proba_positive)\n",
    "quartiles = np.percentile(proba_positive, [25, 50, 75])\n",
    "\n",
    "print(f\"Mean: {mean:.2f}\")\n",
    "print(f\"Standard deviation: {std_dev:.2f}\")\n",
    "print(f\"Median: {median:.2f}\")\n",
    "print(f\"First quartile (Q1): {quartiles[0]:.2f}\")\n",
    "\n",
    "threshold_cut = mean - std_dev\n",
    "print(f\"Cutoff threshold defined as Mean - Standard Deviation: {threshold_cut:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c0d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Create histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(proba_positive, bins=50, kde=True)\n",
    "plt.title('Histogram of Positive Class Probabilities')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Create boxplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(y=proba_positive)\n",
    "plt.title('Boxplot of Positive Class Probabilities predicted by Naive Bayes')\n",
    "plt.ylabel('Predicted Probability')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Create plot with point (circle) and error bars\n",
    "plt.figure(figsize=(6, 8))\n",
    "plt.errorbar(['Positive Probability'], [mean], yerr=[std_dev], fmt='o', capsize=5, color='skyblue', markersize=10)\n",
    "plt.title('Mean and Standard Deviation of Positive Class Probabilities')\n",
    "plt.ylabel('Probability')\n",
    "plt.ylim(0, 1)  # Set y-axis limits between 0 and 1\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97894cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count samples less than threshold_cut\n",
    "count_smaller = (proba_positive < threshold_cut).sum()\n",
    "\n",
    "# Count samples greater than threshold_cut\n",
    "count_larger = (proba_positive >= threshold_cut).sum()\n",
    "\n",
    "print(f\"Number of samples with probability less than {threshold_cut:.2f}: {count_smaller}\")\n",
    "print(f\"Number of samples with probability greater than or equal to {threshold_cut:.2f}: {count_larger}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b75e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes and probabilities for unlabeled data\n",
    "predicted_proba = nb_classifier.predict_proba(X_unlabeled)\n",
    "predicted_classes = nb_classifier.predict(X_unlabeled)\n",
    "\n",
    "\n",
    "# Create DataFrame with unlabeled data and predicted classes\n",
    "unlabeled_predictions_df = X_unlabeled.copy()\n",
    "unlabeled_predictions_df['predicted_classes'] = predicted_classes\n",
    "unlabeled_predictions_df['proba_class_0'] = predicted_proba[:, 0]\n",
    "unlabeled_predictions_df['proba_class_1'] = predicted_proba[:, 1]\n",
    "\n",
    "\n",
    "# Display first rows of new DataFrame and count of predicted classes\n",
    "print(\"DataFrame with unlabeled data, predicted classes and probabilities:\")\n",
    "display(unlabeled_predictions_df.head(10))\n",
    "\n",
    "print(\"\\nCount of predicted classes in unlabeled data:\")\n",
    "print(unlabeled_predictions_df['predicted_classes'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dcdeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column 'new_class' based on cutoff threshold\n",
    "unlabeled_predictions_df['new_class'] = (unlabeled_predictions_df['proba_class_1'] >= threshold_cut).astype(int)\n",
    "\n",
    "# Display first rows of DataFrame with new column\n",
    "print(\"DataFrame with new column 'new_class':\")\n",
    "display(unlabeled_predictions_df.head(1000))\n",
    "\n",
    "# Display value counts in new column\n",
    "print(\"\\nValue counts in new column 'new_class':\")\n",
    "print(unlabeled_predictions_df['new_class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8df37ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_predictions_df = unlabeled_predictions_df.drop(columns=['predicted_classes', 'proba_class_0', 'proba_class_1'])\n",
    "display(unlabeled_predictions_df.head())\n",
    "print(\"\\nColumns after dropping:\")\n",
    "print(unlabeled_predictions_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2769bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of X_labeled DataFrame and reset its index\n",
    "labeled_df = X_labeled.copy().reset_index(drop=True)\n",
    "\n",
    "# Add 'new_class' column from y_labeled\n",
    "labeled_df['new_class'] = y_labeled.reset_index(drop=True)\n",
    "\n",
    "print(labeled_df['new_class'].value_counts())\n",
    "print(unlabeled_predictions_df['new_class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc767a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate labeled and unlabeled dataframes\n",
    "combined_df = pd.concat([labeled_df, unlabeled_predictions_df], ignore_index=True)\n",
    "\n",
    "print(\"Combined DataFrame:\")\n",
    "display(combined_df.head())\n",
    "print(\"\\nValue counts of new_class in combined DataFrame:\")\n",
    "print(combined_df['new_class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be670d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Select relevant columns for analysis from combined_df\n",
    "combined_data_for_analysis = combined_df.drop(columns=[col for col in combined_df.columns if col in ['system:index', 'landcover_classification_2022', '.geo']])\n",
    "\n",
    "# Visualize distributions of numerical features\n",
    "combined_data_for_analysis.hist(bins=30, figsize=(15, 10))\n",
    "plt.suptitle('Distribution of Numerical Features (Combined data with new classes)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize relationships between features using pairplot\n",
    "sns.pairplot(combined_data_for_analysis, hue='new_class', diag_kind='kde')\n",
    "plt.suptitle('Pairplot of Features by New Classes (Combined data with new classes)', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Visualize boxplots for each feature by class\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(combined_data_for_analysis.columns[:-1]): # Exclude the target variable for individual boxplots\n",
    "    plt.subplot(3, 3, i + 1) # Changed from 2x2 to 3x3 grid\n",
    "    sns.boxplot(x='new_class', y=col, data=combined_data_for_analysis)\n",
    "    plt.title(f'Boxplot of {col} by New Classes')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Pearson correlation matrix for all classes in the combined data\n",
    "combined_correlation_matrix_all = combined_data_for_analysis.drop(columns=['new_class']).corr(method='pearson')\n",
    "\n",
    "# Display the correlation matrix for all classes\n",
    "print(\"Pearson Correlation Matrix for all classes (Combined Data):\")\n",
    "display(combined_correlation_matrix_all)\n",
    "\n",
    "# Visualize the correlation matrix for all classes as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(combined_correlation_matrix_all, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Pearson Correlation Matrix - All Classes (Combined Data)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate Pearson correlation matrix for class 0\n",
    "combined_data_correlation_matrix_class_0 = combined_data_for_analysis[combined_data_for_analysis['new_class'] == 0].drop(columns=['new_class']).corr(method='pearson')\n",
    "\n",
    "# Display correlation matrix for class 0\n",
    "print(\"Pearson Correlation Matrix for Class 0:\")\n",
    "display(combined_data_correlation_matrix_class_0)\n",
    "\n",
    "# Visualize correlation matrix for class 0 as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(combined_data_correlation_matrix_class_0, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Pearson Correlation Matrix - New Class 0 (Combined Data)' )\n",
    "plt.show()\n",
    "\n",
    "# Calculate Pearson correlation matrix for class 1\n",
    "combined_data_correlation_matrix_class_1 = combined_data_for_analysis[combined_data_for_analysis['new_class'] == 1].drop(columns=['new_class']).corr(method='pearson')\n",
    "\n",
    "# Display correlation matrix for class 1\n",
    "print(\"\\nPearson Correlation Matrix for Class 1:\")\n",
    "display(combined_data_correlation_matrix_class_1)\n",
    "\n",
    "# Visualize correlation matrix for class 1 as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(combined_data_correlation_matrix_class_1, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Pearson Correlation Matrix - New Class 1 (Combined Data)')\n",
    "plt.show()\n",
    "\n",
    "# Generate basic statistics for the entire combined_data_for_analysis DataFrame\n",
    "print(\"Basic Statistics for the entire dataset:\")\n",
    "display(combined_data_for_analysis.describe())\n",
    "\n",
    "# Generate basic statistics for Class 0\n",
    "print(\"\\nBasic Statistics for Class 0:\")\n",
    "display(combined_data_for_analysis[combined_data_for_analysis['new_class'] == 0].describe())\n",
    "\n",
    "# Generate basic statistics for Class 1\n",
    "print(\"\\nBasic Statistics for Class 1:\")\n",
    "display(combined_data_for_analysis[combined_data_for_analysis['new_class'] == 1].describe())\n",
    "\n",
    "\n",
    "# --- Variance Inflation Factor (VIF) Calculation ---\n",
    "\n",
    "# All classes\n",
    "# Select features for VIF calculation (exclude the target variable 'new_class')\n",
    "X_vif_combined = combined_data_for_analysis.drop(columns=['new_class'])\n",
    "# Add a constant to the features for VIF calculation\n",
    "X_vif_combined = sm.add_constant(X_vif_combined)\n",
    "\n",
    "# Calculate VIF for each feature in the combined data\n",
    "vif_data_combined = pd.DataFrame()\n",
    "vif_data_combined[\"feature\"] = X_vif_combined.columns\n",
    "vif_data_combined[\"VIF\"] = [variance_inflation_factor(X_vif_combined.values, i)\n",
    "                           for i in range(X_vif_combined.shape[1])]\n",
    "# Remove the constant from the VIF table\n",
    "vif_data_combined = vif_data_combined[vif_data_combined[\"feature\"] != \"const\"]\n",
    "print(\"Variance Inflation Factor (VIF) for all features (Combined Data):\")\n",
    "display(vif_data_combined)\n",
    "\n",
    "# Class 0\n",
    "# Select features for VIF calculation for new class 0\n",
    "X_vif_combined_class_0 = combined_data_for_analysis[combined_data_for_analysis['new_class'] == 0].drop(columns=['new_class'])\n",
    "# Add a constant to the features for VIF calculation\n",
    "X_vif_combined_class_0 = sm.add_constant(X_vif_combined_class_0)\n",
    "\n",
    "# Calculate VIF for each feature in new Class 0\n",
    "vif_data_combined_class_0 = pd.DataFrame()\n",
    "vif_data_combined_class_0[\"feature\"] = X_vif_combined_class_0.columns\n",
    "vif_data_combined_class_0[\"VIF\"] = [variance_inflation_factor(X_vif_combined_class_0.values, i)\n",
    "                                   for i in range(X_vif_combined_class_0.shape[1])]\n",
    "# Remove the constant from the VIF table\n",
    "vif_data_combined_class_0 = vif_data_combined_class_0[vif_data_combined_class_0[\"feature\"] != \"const\"]\n",
    "print(\"\\nVariance Inflation Factor (VIF) for features in New Class 0:\")\n",
    "display(vif_data_combined_class_0)\n",
    "\n",
    "# Class 1\n",
    "# Select features for VIF calculation for new class 1\n",
    "X_vif_combined_class_1 = combined_data_for_analysis[combined_data_for_analysis['new_class'] == 1].drop(columns=['new_class'])\n",
    "# Add a constant to the features for VIF calculation\n",
    "X_vif_combined_class_1 = sm.add_constant(X_vif_combined_class_1)\n",
    "\n",
    "# Calculate VIF for each feature in new Class 1\n",
    "vif_data_combined_class_1 = pd.DataFrame()\n",
    "vif_data_combined_class_1[\"feature\"] = X_vif_combined_class_1.columns\n",
    "vif_data_combined_class_1[\"VIF\"] = [variance_inflation_factor(X_vif_combined_class_1.values, i)\n",
    "                                   for i in range(X_vif_combined_class_1.shape[1])]\n",
    "# Remove the constant from the VIF table\n",
    "vif_data_combined_class_1 = vif_data_combined_class_1[vif_data_combined_class_1[\"feature\"] != \"const\"]\n",
    "print(\"\\nVariance Inflation Factor (VIF) for features in New Class 1:\")\n",
    "display(vif_data_combined_class_1)\n",
    "\n",
    "\n",
    "'''VIF Value Interpretation\n",
    "1       No correlation with other variables\n",
    "1–5     Moderate correlation, generally acceptable\n",
    "> 5     Concerning multicollinearity\n",
    "> 10    Very high – should remove or combine variables'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866af321",
   "metadata": {},
   "source": [
    "===ATTENTION: Execute the first part of the code (increase, labeling, data balancing) up to this cell==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435587b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Export final DataFrame to CSV file\n",
    "combined_df.to_csv('combined_dataset.csv', index=False)\n",
    "\n",
    "print(\"DataFrame successfully exported to 'combined_dataset.csv'\")\n",
    "\n",
    "# Download file\n",
    "files.download('combined_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddfc101",
   "metadata": {},
   "source": [
    "===ATTENTION: Execute the second procedure (separability analysis) in the code starting from the cell below.==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff28f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load CSV file to examine data\n",
    "file_path = '/content/drive/MyDrive/floodProjectAssets/dataSeparabilityEvaluation.csv'\n",
    "data_evaluation = pd.read_csv(file_path)\n",
    "\n",
    "# Display first rows of file to see its structure\n",
    "display(data_evaluation.head())\n",
    "\n",
    "# Display number of rows in file\n",
    "print(f\"\\nNumber of rows in file: {data_evaluation.shape[0]}\")\n",
    "\n",
    "# Display value counts in 'cluster' column\n",
    "print(\"\\nValue counts in 'cluster' column:\")\n",
    "print(data_evaluation['cluster'].value_counts())\n",
    "# Display value counts in 'cluster' column\n",
    "print(\"\\nValue counts in 'classification_hand' column:\")\n",
    "print(data_evaluation['classification_hand'].value_counts())\n",
    "# Display value counts in 'cluster' column\n",
    "print(\"\\nValue counts in 'classification' column:\")\n",
    "print(data_evaluation['classification'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31133b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the specified columns\n",
    "data_evaluation_dropped = data_evaluation.drop(columns=['system:index', '.geo'])\n",
    "\n",
    "# Separate the data into three dataframes\n",
    "attributes = data_evaluation_dropped.drop(columns=['classification', 'classification_hand', 'cluster'])\n",
    "\n",
    "data_with_classification = attributes.copy()\n",
    "data_with_classification['classification'] = data_evaluation_dropped['classification']\n",
    "\n",
    "data_with_classification_hand = attributes.copy()\n",
    "data_with_classification_hand['classification_hand'] = data_evaluation_dropped['classification_hand']\n",
    "\n",
    "data_with_cluster = attributes.copy()\n",
    "data_with_cluster['cluster'] = data_evaluation_dropped['cluster']\n",
    "\n",
    "# Display the head of each new dataframe to verify\n",
    "print(\"Data with 'classification':\")\n",
    "display(data_with_classification.head())\n",
    "\n",
    "print(\"\\nData with 'classification_hand':\")\n",
    "display(data_with_classification_hand.head())\n",
    "\n",
    "print(\"\\nData with 'cluster':\")\n",
    "display(data_with_cluster.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def sample_dataframe(dataframe, target_column, max_samples_per_class=10000):\n",
    "    \"\"\"\n",
    "    Samples a DataFrame to a maximum number of samples per class.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input DataFrame.\n",
    "        target_column (str): The name of the target column (classes).\n",
    "        max_samples_per_class (int): The maximum number of samples desired for each class.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The sampled DataFrame.\n",
    "    \"\"\"\n",
    "    sampled_df = pd.DataFrame()\n",
    "    classes = dataframe[target_column].unique()\n",
    "    #print(classes)\n",
    "    #print(len(classes))\n",
    "    for cls in classes:\n",
    "        class_subset = dataframe[dataframe[target_column] == cls]\n",
    "        #print(len(class_subset), '---- ', max_samples_per_class)\n",
    "        if len(class_subset) > max_samples_per_class:\n",
    "            # Randomly sample if the class has more than max_samples_per_class\n",
    "            sampled_class_subset = class_subset.sample(n=max_samples_per_class, random_state=42)\n",
    "            #print('oi 1')\n",
    "        else:\n",
    "            # Keep all samples if the class has less than or equal to max_samples_per_class\n",
    "            sampled_class_subset = class_subset\n",
    "            #print('oi 2')\n",
    "        sampled_df = pd.concat([sampled_df, sampled_class_subset])\n",
    "\n",
    "    # Shuffle the resulting DataFrame\n",
    "    sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return sampled_df\n",
    "\n",
    "# Sample each of the dataframes\n",
    "sampled_data_with_classification = sample_dataframe(data_with_classification, 'classification')\n",
    "sampled_data_with_classification_hand = sample_dataframe(data_with_classification_hand, 'classification_hand')\n",
    "sampled_data_with_cluster = sample_dataframe(data_with_cluster, 'cluster')\n",
    "\n",
    "# Display the value counts for the new sampled dataframes to verify\n",
    "print(\"Value counts for sampled_data_with_classification:\")\n",
    "print(sampled_data_with_classification['classification'].value_counts())\n",
    "\n",
    "print(\"\\nValue counts for sampled_data_with_classification_hand:\")\n",
    "print(sampled_data_with_classification_hand['classification_hand'].value_counts())\n",
    "\n",
    "print(\"\\nValue counts for sampled_data_with_cluster:\")\n",
    "print(sampled_data_with_cluster['cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fd5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Import seaborn for potentially nicer plots\n",
    "\n",
    "def plot_pca_separability_sampled(dataframe, target_column, title_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Applies PCA and plots class separability for a sampled dataframe.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The sampled DataFrame containing attributes and target column.\n",
    "        target_column (str): The name of the target column (classes).\n",
    "        title_suffix (str): Suffix to add to plot title.\n",
    "    \"\"\"\n",
    "    attributes = dataframe.drop(columns=[target_column])\n",
    "    classes = dataframe[target_column]\n",
    "\n",
    "    # Apply PCA with 2 components\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(attributes)\n",
    "\n",
    "    # Plot scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    # Using seaborn for potentially better looking scatter plot and easier hue mapping\n",
    "    sns.scatterplot(x=principal_components[:, 0], y=principal_components[:, 1], hue=classes, palette={0: 'skyblue', 1: 'red'}, alpha=0.6)\n",
    "\n",
    "    plt.title(f'PCA of 2 Components – Class Separation {title_suffix}')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend(title=target_column)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming the sampled dataframes are available and calling the function for each\n",
    "if 'sampled_data_with_classification' in locals() and 'sampled_data_with_classification_hand' in locals() and 'sampled_data_with_cluster' in locals():\n",
    "\n",
    "    # Plot for sampled_data_with_classification\n",
    "    plot_pca_separability_sampled(sampled_data_with_classification, 'classification', \" (Sampled - semi-supervised classification)\")\n",
    "\n",
    "    # Plot for sampled_data_with_classification_hand\n",
    "    plot_pca_separability_sampled(sampled_data_with_classification_hand, 'classification_hand', \" (Sampled - slicing classification by HAND)\")\n",
    "\n",
    "    # Plot for sampled_data_with_cluster\n",
    "    plot_pca_separability_sampled(sampled_data_with_cluster, 'cluster', \" (Sampled - unsupervised classification)\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Sampled DataFrames not found. Please run the cell that creates the sampled dataframes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e885e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import multivariate_normal\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_fdr(X1, X2):\n",
    "    \"\"\"Calculates Fisher's Discriminant Ratio.\"\"\"\n",
    "    mean1 = np.mean(X1, axis=0)\n",
    "    mean2 = np.mean(X2, axis=0)\n",
    "    var1 = np.var(X1, axis=0)\n",
    "    var2 = np.var(X2, axis=0)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    fdr = np.sum(((mean1 - mean2)**2) / (var1 + var2 + 1e-8))\n",
    "    return fdr\n",
    "\n",
    "def calculate_bhattacharyya_distance(X1, X2):\n",
    "    \"\"\"Calculates Bhattacharyya Distance assuming multivariate normal distributions.\"\"\"\n",
    "    mean1 = np.mean(X1, axis=0)\n",
    "    mean2 = np.mean(X2, axis=0)\n",
    "    cov1 = np.cov(X1.T)\n",
    "    cov2 = np.cov(X2.T)\n",
    "\n",
    "    # Add a small value to the diagonal for numerical stability\n",
    "    cov1 += np.eye(cov1.shape[0]) * 1e-6\n",
    "    cov2 += np.eye(cov2.shape[0]) * 1e-6\n",
    "\n",
    "    try:\n",
    "        cov_pooled = (cov1 + cov2) / 2\n",
    "        term1 = 0.125 * np.dot((mean1 - mean2).T, np.linalg.solve(cov_pooled, (mean1 - mean2)))\n",
    "        term2 = 0.5 * np.log(np.linalg.det(cov_pooled) / np.sqrt(np.linalg.det(cov1) * np.linalg.det(cov2)))\n",
    "        b_distance = term1 + term2\n",
    "        return b_distance\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Warning: Could not calculate Bhattacharyya Distance due to singular covariance matrix.\")\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def calculate_mahalanobis_distance(X1, X2):\n",
    "    \"\"\"Calculates the average Mahalanobis distance between points in X1 and X2.\"\"\"\n",
    "    mean1 = np.mean(X1, axis=0)\n",
    "    mean2 = np.mean(X2, axis=0)\n",
    "    cov_pooled = (np.cov(X1.T) + np.cov(X2.T)) / 2\n",
    "\n",
    "    # Add a small value to the diagonal for numerical stability\n",
    "    cov_pooled += np.eye(cov_pooled.shape[0]) * 1e-6\n",
    "\n",
    "    try:\n",
    "        inv_cov_pooled = np.linalg.inv(cov_pooled)\n",
    "        avg_mahalanobis = mahalanobis(mean1, mean2, inv_cov_pooled)\n",
    "        return avg_mahalanobis\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Warning: Could not calculate Mahalanobis Distance due to singular covariance matrix.\")\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def evaluate_separability_indices(dataframe, target_column):\n",
    "    \"\"\"Calculates separability indices for a dataframe.\"\"\"\n",
    "    classes = dataframe[target_column].unique()\n",
    "    if len(classes) != 2:\n",
    "        print(f\"Warning: Separability indices are typically for binary classification. Found {len(classes)} classes.\")\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    class_0_data = dataframe[dataframe[target_column] == classes[0]].drop(columns=[target_column]).values\n",
    "    class_1_data = dataframe[dataframe[target_column] == classes[1]].drop(columns=[target_column]).values\n",
    "\n",
    "    if len(class_0_data) < 2 or len(class_1_data) < 2:\n",
    "         print(\"Warning: Need at least 2 samples in each class to calculate covariance. Skipping index calculation.\")\n",
    "         return np.nan, np.nan\n",
    "\n",
    "\n",
    "    b_distance = calculate_bhattacharyya_distance(class_0_data, class_1_data)\n",
    "    m_distance = calculate_mahalanobis_distance(class_0_data, class_1_data)\n",
    "\n",
    "    return b_distance, m_distance\n",
    "\n",
    "# Assuming the sampled dataframes are available\n",
    "if 'sampled_data_with_classification' in locals() and 'sampled_data_with_classification_hand' in locals() and 'sampled_data_with_cluster' in locals():\n",
    "\n",
    "    print(\"--- Evaluating sampled_data_with_classification ---\")\n",
    "    b_dist_clf, m_dist_clf = evaluate_separability_indices(sampled_data_with_classification, 'classification')\n",
    "    print(f\"Bhattacharyya Distance: {b_dist_clf:.2f}\")\n",
    "    print(f\"Mahalanobis Distance: {m_dist_clf:.2f}\")\n",
    "\n",
    "    print(\"\\n--- Evaluating sampled_data_with_classification_hand ---\")\n",
    "    b_dist_hand, m_dist_hand = evaluate_separability_indices(sampled_data_with_classification_hand, 'classification_hand')\n",
    "    print(f\"Bhattacharyya Distance: {b_dist_hand:.2f}\")\n",
    "    print(f\"Mahalanobis Distance: {m_dist_hand:.2f}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Evaluating sampled_data_with_cluster ---\")\n",
    "    b_dist_cluster, m_dist_cluster = evaluate_separability_indices(sampled_data_with_cluster, 'cluster')\n",
    "    print(f\"Bhattacharyya Distance: {b_dist_cluster:.2f}\")\n",
    "    print(f\"Mahalanobis Distance: {m_dist_cluster:.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Sampled DataFrames not found. Please run the cell that creates the sampled dataframes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the sampled dataframes are available\n",
    "if 'sampled_data_with_classification' in locals() and 'sampled_data_with_classification_hand' in locals() and 'sampled_data_with_cluster' in locals():\n",
    "\n",
    "    print(\"--- Evaluating sampled_data_with_classification ---\")\n",
    "    X_clf = sampled_data_with_classification.drop(columns=['classification']).values\n",
    "    labels_clf = sampled_data_with_classification['classification'].values\n",
    "    try:\n",
    "        silhouette_avg_clf = silhouette_score(X_clf, labels_clf)\n",
    "        print(f\"Silhouette Score: {silhouette_avg_clf:.2f}\")\n",
    "    except ValueError as e:\n",
    "         print(f\"Could not calculate Silhouette Score: {e}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Evaluating sampled_data_with_classification_hand ---\")\n",
    "    X_hand = sampled_data_with_classification_hand.drop(columns=['classification_hand']).values\n",
    "    labels_hand = sampled_data_with_classification_hand['classification_hand'].values\n",
    "    try:\n",
    "        silhouette_avg_hand = silhouette_score(X_hand, labels_hand)\n",
    "        print(f\"Silhouette Score: {silhouette_avg_hand:.2f}\")\n",
    "    except ValueError as e:\n",
    "         print(f\"Could not calculate Silhouette Score: {e}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Evaluating sampled_data_with_cluster ---\")\n",
    "    X_cluster = sampled_data_with_cluster.drop(columns=['cluster']).values\n",
    "    labels_cluster = sampled_data_with_cluster['cluster'].values\n",
    "    try:\n",
    "        silhouette_avg_cluster = silhouette_score(X_cluster, labels_cluster)\n",
    "        print(f\"Silhouette Score: {silhouette_avg_cluster:.2f}\")\n",
    "    except ValueError as e:\n",
    "         print(f\"Could not calculate Silhouette Score: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Sampled DataFrames not found. Please run the cell that creates the sampled dataframes.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
